{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split \n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import WordPunctTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the unique words and no of sentences(num_recs)\n",
    "cwd = os.getcwd()\n",
    "direct = os.path.join(cwd, \"si650winter11\")\n",
    "ftrain = open(os.path.join(direct, \"training.txt\"), 'r')\n",
    "for line in ftrain:\n",
    "    label, sent = line.strip().split('\\t')\n",
    "#     print(label)\n",
    "    words = nltk.WordPunctTokenizer().tokenize(sent.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += word_freqs[word]\n",
    "    num_recs += 1\n",
    "ftrain.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "2308\n",
      "7086\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "# print(word_freqs)\n",
    "print(len(word_freqs))\n",
    "print(num_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words in the sentence (maxlen) allows us to set a fixed sequence length and zero pad shorter sentences and truncate longer sentences to that length as appropriate. Even though RNNs handle variable sequence length, this is usually achieved either by padding and truncating as above, or by grouping the inputs in different batches by sequence length.\n",
    "For the latter approach, Keras recommends using batches of size one.\n",
    "\n",
    "*type(word_freqs)*\n",
    "- *collections.Counter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary size=2000 + 2,, unknown word and pseudo-Word\n",
    "max_features = 2000\n",
    "#maximum length of a sentence..i.e. to pad allt he sentences to the same length\n",
    "max_sent_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(max_features, len(word_freqs)) + 2\n",
    "word2index = {w[0]:i+2 for i,w in enumerate(word_freqs.most_common(max_features))}\n",
    "word2index[\"pad\"] = 0\n",
    "word2index[\"unk\"] = 1\n",
    "index2word = {i: w for i, w in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now pad our input sentences to word-index sequences with value max_sent_len = 40\n",
    "x = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs), )\n",
    "i = 0 \n",
    "ftrain = open(os.path.join(direct, \"training.txt\"), 'r')\n",
    "for line in ftrain:\n",
    "    label, sent = line.strip().split('\\t')\n",
    "    words = nltk.WordPunctTokenizer().tokenize(sent.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"unk\"])\n",
    "    x[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "x = sequence.pad_sequences(x, maxlen = max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_layer_size = 64\n",
    "batch_size = 32\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-0befbf98da3b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-0befbf98da3b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    model.add(SpatialDropout1D(Dropout(0.2)))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_size, input_length = max_sent_len)\n",
    "model.add(SpatialDropout1D(Dropout(0.2)))\n",
    "model.add(LSTM(hidden_layer_size, dropout=0.2, recurrent_dropout =))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
